DECISION TREE RESULTS
Training Set 1:
Decision Tree training time: 0.37401485443115234
Best Classifier Chosen: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
                       max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=6,
                       min_weight_fraction_leaf=0.0, presort=False,
                       random_state=13, splitter='best')
In sample accuracy for Decision Tree: 0.93
In sample precision for Decision Tree: 0.9333500000000001
Decision Tree insample query time: 0.0019905567169189453
Out of sample accuracy for Decision Tree: 0.5743202416918429
Out of sample precision for Decision Tree: 0.5732808333464795
Decision Tree out of sample query time: 0.09189963340759277
END OF ITERATION
----------------------------------------------------------------------------------
Training Set 2:
Decision Tree training time: 2.0362842082977295
Best Classifier Chosen: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,
                       max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, presort=False,
                       random_state=13, splitter='best')
In sample accuracy for Decision Tree: 0.744
In sample precision for Decision Tree: 0.7690594596146668
Decision Tree insample query time: 0.013930559158325195
Out of sample accuracy for Decision Tree: 0.6734138972809668
Out of sample precision for Decision Tree: 0.6854625927469038
Decision Tree out of sample query time: 0.09075689315795898
END OF ITERATION
----------------------------------------------------------------------------------
Training Set 3:
Decision Tree training time: 5.155242681503296
Best Classifier Chosen: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,
                       max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, presort=False,
                       random_state=13, splitter='best')
In sample accuracy for Decision Tree: 0.7348
In sample precision for Decision Tree: 0.7480303855302818
Decision Tree insample query time: 0.03490710258483887
Out of sample accuracy for Decision Tree: 0.7090634441087613
Out of sample precision for Decision Tree: 0.7147344442986887
Decision Tree out of sample query time: 0.09075736999511719
END OF ITERATION
----------------------------------------------------------------------------------
Training Set 4:
Decision Tree training time: 10.838273763656616
Best Classifier Chosen: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,
                       max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=4,
                       min_weight_fraction_leaf=0.0, presort=False,
                       random_state=13, splitter='best')
In sample accuracy for Decision Tree: 0.8768
In sample precision for Decision Tree: 0.8771560291591033
Decision Tree insample query time: 0.0688161849975586
Out of sample accuracy for Decision Tree: 0.7318731117824774
Out of sample precision for Decision Tree: 0.7229573014889821
Decision Tree out of sample query time: 0.09075665473937988
END OF ITERATION
----------------------------------------------------------------------------------
Training Set 5:
Decision Tree training time: 24.75224995613098
Best Classifier Chosen: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,
                       max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, presort=False,
                       random_state=13, splitter='best')
In sample accuracy for Decision Tree: 0.8501
In sample precision for Decision Tree: 0.8498865330881908
Decision Tree insample query time: 0.1406242847442627
Out of sample accuracy for Decision Tree: 0.7478851963746224
Out of sample precision for Decision Tree: 0.7387635777566355
Decision Tree out of sample query time: 0.09175467491149902
END OF ITERATION
----------------------------------------------------------------------------------
Training Set 6:
Decision Tree training time: 33.96316075325012
Best Classifier Chosen: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,
                       max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, presort=False,
                       random_state=13, splitter='best')
In sample accuracy for Decision Tree: 0.8485637743711862
In sample precision for Decision Tree: 0.85041424783308
Decision Tree insample query time: 0.18652772903442383
Out of sample accuracy for Decision Tree: 0.7593655589123867
Out of sample precision for Decision Tree: 0.7504970785486438
Decision Tree out of sample query time: 0.09571456909179688
END OF ITERATION
----------------------------------------------------------------------------------
DECISION TREE W/ BOOSTING RESULTS
Training Set 1:
Boosted Decision Tree training time: 5.016566753387451
Best Classifier Chosen: AdaBoostClassifier(algorithm='SAMME.R',
                   base_estimator=DecisionTreeClassifier(class_weight=None,
                                                         criterion='gini',
                                                         max_depth=10,
                                                         max_features=None,
                                                         max_leaf_nodes=None,
                                                         min_impurity_decrease=0.0,
                                                         min_impurity_split=None,
                                                         min_samples_leaf=1,
                                                         min_samples_split=10,
                                                         min_weight_fraction_leaf=0.0,
                                                         presort=False,
                                                         random_state=13,
                                                         splitter='best'),
                   learning_rate=1.0, n_estimators=50, random_state=13)
In sample accuracy for Boosted Decision Tree: 1.0
In sample precision for Boosted Decision Tree: 1.0
Boosted Decision Tree insample query time: 0.04886937141418457
Out of sample accuracy for Boosted Decision Tree: 0.666012084592145
Out of sample precision for Boosted Decision Tree: 0.6475549172985049
Boosted Decision Tree out of sample query time: 2.868875741958618
END OF ITERATION
----------------------------------------------------------------------------------
Training Set 2:
Boosted Decision Tree training time: 41.335180044174194
Best Classifier Chosen: AdaBoostClassifier(algorithm='SAMME.R',
                   base_estimator=DecisionTreeClassifier(class_weight=None,
                                                         criterion='gini',
                                                         max_depth=10,
                                                         max_features=None,
                                                         max_leaf_nodes=None,
                                                         min_impurity_decrease=0.0,
                                                         min_impurity_split=None,
                                                         min_samples_leaf=1,
                                                         min_samples_split=10,
                                                         min_weight_fraction_leaf=0.0,
                                                         presort=False,
                                                         random_state=13,
                                                         splitter='best'),
                   learning_rate=1.0, n_estimators=50, random_state=13)
